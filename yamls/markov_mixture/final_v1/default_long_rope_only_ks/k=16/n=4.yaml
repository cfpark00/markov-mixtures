data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 16
  - 16
  - 16
  - 16
  l: 512
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_rope_only_ks/k=16/n=4
mask_loss: false
memo:
  flops_per_step: 92610232320
model_params:
  gpt_config:
    in_size: 676
    n_embd: 64
    n_head: 4
    n_layer: 2
    pos_embed: false
    rope: true
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 172767
  save_opt: false
  save_steps:
  - 29
  - 35
  - 42
  - 50
  - 60
  - 72
  - 86
  - 103
  - 123
  - 147
  - 175
  - 209
  - 250
  - 298
  - 356
  - 424
  - 506
  - 604
  - 721
  - 861
  - 1027
  - 1226
  - 1463
  - 1746
  - 2084
  - 2486
  - 2967
  - 3541
  - 4225
  - 5042
  - 6016
  - 7179
  - 8567
  - 10223
  - 12199
  - 14557
  - 17370
  - 20728
  - 24734
  - 29515
  - 35219
  - 42026
  - 50149
  - 59842
  - 71408
  - 85210
  - 101680
  - 121332
  - 144783
  - 172767
wandb:
  group: final_v1/default_long_rope_only_ks
  name: k=16_n=4
  project: markov_mixture
