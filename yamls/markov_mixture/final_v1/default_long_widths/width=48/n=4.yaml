data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  l: 512
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_widths/width=48/n=4
mask_loss: false
memo:
  flops_per_step: 62813896704
model_params:
  gpt_config:
    in_size: 676
    n_embd: 48
    n_head: 4
    n_layer: 2
    rope: false
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 159200
  save_opt: false
  save_steps:
  - 29
  - 35
  - 42
  - 50
  - 60
  - 71
  - 85
  - 102
  - 121
  - 144
  - 172
  - 205
  - 245
  - 291
  - 347
  - 414
  - 493
  - 588
  - 700
  - 834
  - 994
  - 1184
  - 1410
  - 1680
  - 2002
  - 2385
  - 2841
  - 3385
  - 4032
  - 4804
  - 5723
  - 6817
  - 8121
  - 9675
  - 11526
  - 13731
  - 16357
  - 19486
  - 23214
  - 27655
  - 32945
  - 39247
  - 46754
  - 55698
  - 66352
  - 79045
  - 94165
  - 112178
  - 133636
  - 159199
wandb:
  group: final_v1/default_long_widths
  name: width=48_n=4
  project: markov_mixture
