data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  l: 512
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_widths/width=256/n=4
mask_loss: false
memo:
  flops_per_step: 827049639936
model_params:
  gpt_config:
    in_size: 676
    n_embd: 256
    n_head: 4
    n_layer: 2
    rope: false
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 12091
  save_opt: false
  save_steps:
  - 29
  - 33
  - 38
  - 43
  - 48
  - 55
  - 62
  - 70
  - 79
  - 90
  - 102
  - 115
  - 130
  - 147
  - 166
  - 188
  - 212
  - 240
  - 271
  - 307
  - 347
  - 392
  - 443
  - 501
  - 566
  - 640
  - 723
  - 817
  - 924
  - 1044
  - 1180
  - 1334
  - 1508
  - 1705
  - 1927
  - 2178
  - 2461
  - 2782
  - 3144
  - 3554
  - 4017
  - 4540
  - 5131
  - 5800
  - 6555
  - 7409
  - 8374
  - 9465
  - 10697
  - 12091
wandb:
  group: final_v1/default_long_widths
  name: width=256_n=4
  project: markov_mixture
