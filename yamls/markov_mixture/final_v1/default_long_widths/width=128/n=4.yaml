data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  l: 512
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_widths/width=128/n=4
mask_loss: false
memo:
  flops_per_step: 260113956864
model_params:
  gpt_config:
    in_size: 676
    n_embd: 128
    n_head: 4
    n_layer: 2
    rope: false
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 38444
  save_opt: false
  save_steps:
  - 29
  - 34
  - 40
  - 46
  - 53
  - 62
  - 72
  - 83
  - 96
  - 111
  - 129
  - 149
  - 173
  - 200
  - 231
  - 268
  - 310
  - 359
  - 415
  - 481
  - 556
  - 644
  - 745
  - 862
  - 998
  - 1155
  - 1336
  - 1547
  - 1790
  - 2071
  - 2397
  - 2774
  - 3211
  - 3715
  - 4300
  - 4976
  - 5758
  - 6664
  - 7712
  - 8924
  - 10328
  - 11952
  - 13831
  - 16006
  - 18523
  - 21435
  - 24806
  - 28706
  - 33220
  - 38443
wandb:
  group: final_v1/default_long_widths
  name: width=128_n=4
  project: markov_mixture
