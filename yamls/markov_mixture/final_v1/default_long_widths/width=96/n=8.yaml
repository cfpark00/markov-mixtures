data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  - 10
  - 10
  - 10
  - 10
  l: 512
  ps:
  - 0.09622888483605832
  - 0.2442626914976671
  - 0.18806786558416266
  - 0.1538105944677268
  - 0.04008515784535251
  - 0.0400789607764453
  - 0.014923157613654134
  - 0.22254268737893326
  transition_paramss:
  - random_state: 44131
    type: dense
  - random_state: 60263
    type: dense
  - random_state: 16023
    type: dense
  - random_state: 41090
    type: dense
  - random_state: 67221
    type: dense
  - random_state: 64820
    type: dense
  - random_state: 769
    type: dense
  - random_state: 59735
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_widths/width=96/n=8
mask_loss: false
memo:
  flops_per_step: 166698418176
model_params:
  gpt_config:
    in_size: 676
    n_embd: 96
    n_head: 4
    n_layer: 2
    rope: false
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 59988
  save_opt: false
  save_steps:
  - 29
  - 35
  - 40
  - 47
  - 55
  - 65
  - 76
  - 88
  - 103
  - 121
  - 141
  - 165
  - 192
  - 225
  - 263
  - 307
  - 358
  - 419
  - 489
  - 571
  - 667
  - 779
  - 910
  - 1063
  - 1241
  - 1449
  - 1692
  - 1977
  - 2308
  - 2696
  - 3148
  - 3676
  - 4293
  - 5014
  - 5855
  - 6838
  - 7985
  - 9325
  - 10890
  - 12717
  - 14851
  - 17343
  - 20253
  - 23651
  - 27620
  - 32255
  - 37667
  - 43987
  - 51368
  - 59988
wandb:
  group: final_v1/default_long_widths
  name: width=96_n=8
  project: markov_mixture
