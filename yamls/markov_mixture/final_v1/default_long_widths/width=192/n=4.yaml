data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  l: 512
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_widths/width=192/n=4
mask_loss: false
memo:
  flops_per_step: 504927092736
model_params:
  gpt_config:
    in_size: 676
    n_embd: 192
    n_head: 4
    n_layer: 2
    rope: false
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 19804
  save_opt: false
  save_steps:
  - 29
  - 34
  - 39
  - 44
  - 50
  - 58
  - 66
  - 75
  - 86
  - 98
  - 112
  - 128
  - 147
  - 167
  - 191
  - 218
  - 249
  - 285
  - 325
  - 371
  - 424
  - 484
  - 553
  - 631
  - 721
  - 823
  - 940
  - 1073
  - 1225
  - 1399
  - 1597
  - 1823
  - 2082
  - 2377
  - 2713
  - 3098
  - 3537
  - 4038
  - 4610
  - 5264
  - 6009
  - 6861
  - 7833
  - 8943
  - 10210
  - 11656
  - 13308
  - 15193
  - 17346
  - 19804
wandb:
  group: final_v1/default_long_widths
  name: width=192_n=4
  project: markov_mixture
