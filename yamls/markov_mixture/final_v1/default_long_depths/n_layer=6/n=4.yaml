data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  l: 512
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_depths/n_layer=6/n=4
mask_loss: false
memo:
  flops_per_step: 277830696960
model_params:
  gpt_config:
    in_size: 676
    n_embd: 64
    n_head: 4
    n_layer: 6
    rope: false
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 35993
  save_opt: false
  save_steps:
  - 29
  - 34
  - 40
  - 46
  - 53
  - 61
  - 71
  - 82
  - 95
  - 110
  - 127
  - 147
  - 170
  - 196
  - 227
  - 262
  - 303
  - 351
  - 405
  - 468
  - 541
  - 626
  - 723
  - 836
  - 966
  - 1117
  - 1291
  - 1491
  - 1724
  - 1992
  - 2302
  - 2661
  - 3075
  - 3554
  - 4108
  - 4747
  - 5486
  - 6340
  - 7328
  - 8468
  - 9787
  - 11311
  - 13072
  - 15107
  - 17459
  - 20177
  - 23318
  - 26948
  - 31144
  - 35992
wandb:
  group: final_v1/default_long_depths
  name: n_layer=6_n=4
  project: markov_mixture
