data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  - 10
  - 10
  - 10
  - 10
  l: 512
  ps:
  - 0.09622888483605832
  - 0.2442626914976671
  - 0.18806786558416266
  - 0.1538105944677268
  - 0.04008515784535251
  - 0.0400789607764453
  - 0.014923157613654134
  - 0.22254268737893326
  transition_paramss:
  - random_state: 44131
    type: dense
  - random_state: 60263
    type: dense
  - random_state: 16023
    type: dense
  - random_state: 41090
    type: dense
  - random_state: 67221
    type: dense
  - random_state: 64820
    type: dense
  - random_state: 769
    type: dense
  - random_state: 59735
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_rope_only_nheads/nhead=1/n=8
mask_loss: false
memo:
  flops_per_step: 90798292992
model_params:
  gpt_config:
    in_size: 676
    n_embd: 64
    n_head: 1
    n_layer: 2
    pos_embed: false
    rope: true
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 110134
  save_opt: false
  save_steps:
  - 29
  - 35
  - 41
  - 49
  - 58
  - 69
  - 81
  - 96
  - 114
  - 135
  - 160
  - 189
  - 223
  - 264
  - 313
  - 370
  - 437
  - 517
  - 611
  - 723
  - 855
  - 1011
  - 1195
  - 1413
  - 1671
  - 1976
  - 2336
  - 2763
  - 3267
  - 3862
  - 4567
  - 5400
  - 6384
  - 7549
  - 8925
  - 10553
  - 12478
  - 14753
  - 17444
  - 20625
  - 24387
  - 28834
  - 34092
  - 40310
  - 47661
  - 56353
  - 66629
  - 78780
  - 93147
  - 110133
wandb:
  group: final_v1/default_long_rope_only_nheads
  name: nhead=1_n=8
  project: markov_mixture
