data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 32
  - 32
  - 32
  - 32
  - 32
  - 32
  - 32
  - 32
  l: 512
  ps:
  - 0.09622888483605832
  - 0.2442626914976671
  - 0.18806786558416266
  - 0.1538105944677268
  - 0.04008515784535251
  - 0.0400789607764453
  - 0.014923157613654134
  - 0.22254268737893326
  transition_paramss:
  - random_state: 44131
    type: dense
  - random_state: 60263
    type: dense
  - random_state: 16023
    type: dense
  - random_state: 41090
    type: dense
  - random_state: 67221
    type: dense
  - random_state: 64820
    type: dense
  - random_state: 769
    type: dense
  - random_state: 59735
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_ks/k=32/n=8
mask_loss: false
memo:
  flops_per_step: 92610232320
model_params:
  gpt_config:
    in_size: 676
    n_embd: 64
    n_head: 4
    n_layer: 2
    rope: false
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 345534
  save_opt: false
  save_steps:
  - 29
  - 36
  - 43
  - 53
  - 64
  - 77
  - 94
  - 114
  - 138
  - 167
  - 202
  - 244
  - 296
  - 358
  - 434
  - 525
  - 635
  - 769
  - 931
  - 1127
  - 1364
  - 1650
  - 1997
  - 2418
  - 2926
  - 3542
  - 4286
  - 5188
  - 6279
  - 7599
  - 9197
  - 11131
  - 13472
  - 16305
  - 19733
  - 23883
  - 28905
  - 34984
  - 42340
  - 51243
  - 62019
  - 75060
  - 90843
  - 109946
  - 133065
  - 161046
  - 194910
  - 235896
  - 285499
  - 345534
wandb:
  group: final_v1/default_long_ks
  name: k=32_n=8
  project: markov_mixture
