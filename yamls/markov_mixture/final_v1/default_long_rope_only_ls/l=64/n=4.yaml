data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  l: 64
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_rope_only_ls/l=64/n=4
mask_loss: false
memo:
  flops_per_step: 5674893312
model_params:
  gpt_config:
    block_size: 1024
    in_size: 676
    n_embd: 64
    n_head: 4
    n_layer: 2
    pos_embed: false
    rope: true
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 1762147
  save_opt: false
  save_steps:
  - 29
  - 37
  - 46
  - 58
  - 73
  - 91
  - 115
  - 144
  - 180
  - 225
  - 282
  - 352
  - 441
  - 552
  - 691
  - 864
  - 1082
  - 1354
  - 1694
  - 2119
  - 2652
  - 3318
  - 4152
  - 5195
  - 6500
  - 8132
  - 10175
  - 12731
  - 15930
  - 19931
  - 24938
  - 31202
  - 39040
  - 48847
  - 61117
  - 76470
  - 95679
  - 119713
  - 149784
  - 187410
  - 234486
  - 293388
  - 367085
  - 459295
  - 574668
  - 719022
  - 899637
  - 1125621
  - 1408371
  - 1762146
wandb:
  group: final_v1/default_long_rope_only_ls
  name: l=64_n=4
  project: markov_mixture
