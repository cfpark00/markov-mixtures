data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  l: 1024
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_rope_only_ls/l=1024/n=4
mask_loss: false
memo:
  flops_per_step: 293131517952
model_params:
  gpt_config:
    block_size: 1024
    in_size: 676
    n_embd: 64
    n_head: 4
    n_layer: 2
    pos_embed: false
    rope: true
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 34114
  save_opt: false
  save_steps:
  - 29
  - 34
  - 39
  - 46
  - 53
  - 61
  - 71
  - 81
  - 94
  - 109
  - 126
  - 145
  - 168
  - 194
  - 223
  - 258
  - 298
  - 344
  - 397
  - 459
  - 530
  - 612
  - 706
  - 815
  - 941
  - 1086
  - 1254
  - 1448
  - 1672
  - 1930
  - 2228
  - 2572
  - 2969
  - 3428
  - 3958
  - 4569
  - 5274
  - 6089
  - 7029
  - 8115
  - 9368
  - 10814
  - 12484
  - 14412
  - 16638
  - 19207
  - 22173
  - 25597
  - 29550
  - 34113
wandb:
  group: final_v1/default_long_rope_only_ls
  name: l=1024_n=4
  project: markov_mixture
