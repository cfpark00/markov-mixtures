data_params:
  batch_size: 128
  fast_dataset: true
  ks:
  - 10
  - 10
  - 10
  - 10
  l: 2048
  ps:
  - 0.1410215567872302
  - 0.3579622176722185
  - 0.27560979462158475
  - 0.22540643091896656
  transition_paramss:
  - random_state: 54886
    type: dense
  - random_state: 6265
    type: dense
  - random_state: 82386
    type: dense
  - random_state: 37194
    type: dense
  two_token_input: false
exp_dir: ./data/markov_mixture/final_v1/default_long_rope_only_ls/l=2048/n=4
mask_loss: false
memo:
  flops_per_step: 1017907249152
model_params:
  gpt_config:
    block_size: 2048
    in_size: 676
    n_embd: 64
    n_head: 4
    n_layer: 2
    pos_embed: false
    rope: true
    tokenized: true
  model_type: transformer
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    lr: 0.0006
    weight_decay: 0.1
  optimizer_type: AdamW
seed: 0
task: markov_mixture
training_params:
  n_steps: 9824
  save_opt: false
  save_steps:
  - 29
  - 33
  - 37
  - 42
  - 48
  - 54
  - 60
  - 68
  - 77
  - 86
  - 97
  - 110
  - 123
  - 139
  - 156
  - 176
  - 198
  - 223
  - 251
  - 283
  - 318
  - 358
  - 403
  - 454
  - 511
  - 575
  - 648
  - 729
  - 821
  - 924
  - 1039
  - 1170
  - 1317
  - 1482
  - 1668
  - 1877
  - 2113
  - 2378
  - 2677
  - 3012
  - 3390
  - 3816
  - 4295
  - 4834
  - 5440
  - 6123
  - 6891
  - 7755
  - 8728
  - 9823
wandb:
  group: final_v1/default_long_rope_only_ls
  name: l=2048_n=4
  project: markov_mixture
