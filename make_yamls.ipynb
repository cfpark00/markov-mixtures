{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=utils.Tokenizer_v2()\n",
    "n_tokens=tokenizer.get_n_tokens()\n",
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"exp_dir\":\"./data/\",\n",
    "    \"wandb\":{\"project\":\"markov_mixture\"},\n",
    "    \"seed\":0,\n",
    "    \"task\":\"markov_mixture\",\n",
    "    \"mask_loss\":False,\n",
    "\n",
    "    \"data_params\":{\n",
    "        \"k\":5,\n",
    "        \"transition_params\":{\"type\":\"sparse\",\"sparsity\":0.5,\"random_state\":42},\n",
    "        \"l\":128,\n",
    "        \"batch_size\":128,\n",
    "    },\n",
    "    \"model_params\":{\n",
    "        \"model_type\":\"transformer\",\n",
    "        \"gpt_config\":{\n",
    "            \"tokenized\":True,\n",
    "            \"in_size\":n_tokens,\n",
    "            \"n_embd\":256,\n",
    "            \"n_head\":8,\n",
    "            \"n_layer\":6,\n",
    "            \"rope\":False,\n",
    "        },\n",
    "        \"optimizer_type\":\"AdamW\",\n",
    "        \"optimizer_params\":{\n",
    "                    \"lr\":6e-4,\n",
    "                    \"weight_decay\":1e-1,\n",
    "                    \"betas\":[0.9,0.95]\n",
    "        },\n",
    "    },\n",
    "    \"training_params\":{\n",
    "        \"n_steps\":100_000,\n",
    "        \"save_steps\":[5000,10_000,20_000, 40_000, 60_000, 80_000, 100_000],\n",
    "        \"save_opt\":False,\n",
    "    },\n",
    "}\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_steps(n_steps,step_first=30,n_ckpts=50):\n",
    "    return np.logspace(np.log10(step_first),np.log10(n_steps),n_ckpts,base=10).astype(int).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variants (rope, no mlp, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=[4,8,16,32,64,128,256,512,1024,2048]\n",
    "k=10\n",
    "l=512\n",
    "batch_size=128\n",
    "seed=42\n",
    "for variant in [\"default_long\",\"lte_long\",\"default_long_rope_only\",\"lte_long_rope_only\",\"default_long_rope_only_nomlp\"]:\n",
    "    flops_total=1e15 if \"long\" not in variant else 1e16\n",
    "    n_embd=64\n",
    "    n_head=4\n",
    "    n_layer=2\n",
    "    model_name=variant\n",
    "    group_name=f\"final_v1/{variant}\"\n",
    "    for n in ns:\n",
    "        np.random.seed(seed)\n",
    "        ks=[k]*n\n",
    "        ps=np.random.random(n)\n",
    "        ps/=ps.sum()\n",
    "        ps=ps.tolist()\n",
    "        rss=np.random.randint(0,100000,n).tolist()\n",
    "\n",
    "        exp_name=f\"markov_mixture/{group_name}/{model_name}/n={n}\"#dense,2,3\n",
    "        config_=copy.deepcopy(config)\n",
    "        config_[\"exp_dir\"]=f\"./data/{exp_name}\"\n",
    "        config_[\"wandb\"]={\"project\":\"markov_mixture\",\n",
    "        \"group\":group_name,\n",
    "        \"name\":f\"{model_name}_n={n}\"}\n",
    "\n",
    "        config_[\"data_params\"]={\n",
    "            \"ks\":ks,\n",
    "            \"ps\":ps,\n",
    "            \"transition_paramss\":[{\"type\":\"dense\",\"random_state\":rs} for rs in rss],\n",
    "            \"l\":l,\n",
    "            \"batch_size\":batch_size,\n",
    "            \"fast_dataset\":True,\n",
    "            \"two_token_input\":False if \"lte\" not in variant else True,\n",
    "        }\n",
    "        config_[\"model_params\"][\"gpt_config\"][\"n_embd\"]=n_embd\n",
    "        config_[\"model_params\"][\"gpt_config\"][\"n_head\"]=n_head\n",
    "        config_[\"model_params\"][\"gpt_config\"][\"n_layer\"]=n_layer\n",
    "        config_[\"model_params\"][\"gpt_config\"][\"mlp\"]=False if \"nomlp\" in variant else True\n",
    "        if \"rope_only\" in variant:\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"rope\"]=True\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"pos_embed\"]=False\n",
    "\n",
    "\n",
    "        ## set n_steps\n",
    "        flops_per_step=3*(batch_size)*utils.get_nano_gpt_forward_flops(gpt_config=config_[\"model_params\"][\"gpt_config\"],seq_len=l)\n",
    "        n_steps=int(flops_total/flops_per_step)\n",
    "        config_[\"memo\"]={\"flops_per_step\":flops_per_step,}\n",
    "        config_[\"training_params\"][\"n_steps\"]=n_steps\n",
    "        config_[\"training_params\"][\"save_steps\"]=get_ckpt_steps(n_steps)\n",
    "\n",
    "        yaml_path=os.path.join(f\"./yamls/{exp_name}.yaml\")\n",
    "        fol=os.path.dirname(yaml_path)\n",
    "        os.makedirs(fol,exist_ok=True)\n",
    "        yaml.dump(config_, open(yaml_path, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=[4,8,16,32,64,128,256,512,1024,2048]\n",
    "k=10\n",
    "l=512\n",
    "batch_size=128\n",
    "seed=42\n",
    "for width in [32,48,64,96,128,192,256]:\n",
    "    for variant in [\"default_long\",\"default_long_rope_only\"]:\n",
    "        flops_total=1e15 if \"long\" not in variant else 1e16\n",
    "        n_embd=width\n",
    "        n_head=4\n",
    "        n_layer=2\n",
    "        model_name=f\"width={width}\"\n",
    "        group_name=f\"final_v1/{variant}_widths\"\n",
    "        for n in ns:\n",
    "            np.random.seed(seed)\n",
    "            ks=[k]*n\n",
    "            ps=np.random.random(n)\n",
    "            ps/=ps.sum()\n",
    "            ps=ps.tolist()\n",
    "            rss=np.random.randint(0,100000,n).tolist()\n",
    "\n",
    "            exp_name=f\"markov_mixture/{group_name}/{model_name}/n={n}\"#dense,2,3\n",
    "            config_=copy.deepcopy(config)\n",
    "            config_[\"exp_dir\"]=f\"./data/{exp_name}\"\n",
    "            config_[\"wandb\"]={\"project\":\"markov_mixture\",\n",
    "            \"group\":group_name,\n",
    "            \"name\":f\"{model_name}_n={n}\"}\n",
    "\n",
    "            config_[\"data_params\"]={\n",
    "                \"ks\":ks,\n",
    "                \"ps\":ps,\n",
    "                \"transition_paramss\":[{\"type\":\"dense\",\"random_state\":rs} for rs in rss],\n",
    "                \"l\":l,\n",
    "                \"batch_size\":batch_size,\n",
    "                \"fast_dataset\":True,\n",
    "                \"two_token_input\":False if \"lte\" not in variant else True,\n",
    "            }\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_embd\"]=n_embd\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_head\"]=n_head\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_layer\"]=n_layer\n",
    "            if \"rope_only\" in variant:\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"rope\"]=True\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"pos_embed\"]=False\n",
    "\n",
    "\n",
    "            ## set n_steps\n",
    "            flops_per_step=3*(batch_size)*utils.get_nano_gpt_forward_flops(gpt_config=config_[\"model_params\"][\"gpt_config\"],seq_len=l)\n",
    "            n_steps=int(flops_total/flops_per_step)\n",
    "            config_[\"memo\"]={\"flops_per_step\":flops_per_step,}\n",
    "            config_[\"training_params\"][\"n_steps\"]=n_steps\n",
    "            config_[\"training_params\"][\"save_steps\"]=get_ckpt_steps(n_steps)\n",
    "\n",
    "            yaml_path=os.path.join(f\"./yamls/{exp_name}.yaml\")\n",
    "            fol=os.path.dirname(yaml_path)\n",
    "            os.makedirs(fol,exist_ok=True)\n",
    "            yaml.dump(config_, open(yaml_path, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=[4,8,16,32,64,128,256,512,1024,2048]\n",
    "k=10\n",
    "l=512\n",
    "batch_size=128\n",
    "seed=42\n",
    "for n_layer in [1,2,4,6,8,12,16]:\n",
    "    for variant in [\"default_long\",\"default_long_rope_only\"]:\n",
    "        flops_total=1e15 if \"long\" not in variant else 1e16\n",
    "        n_embd=64\n",
    "        n_head=4\n",
    "        n_layer=n_layer\n",
    "        model_name=f\"n_layer={n_layer}\"\n",
    "        group_name=f\"final_v1/{variant}_depths\"\n",
    "        for n in ns:\n",
    "            np.random.seed(seed)\n",
    "            ks=[k]*n\n",
    "            ps=np.random.random(n)\n",
    "            ps/=ps.sum()\n",
    "            ps=ps.tolist()\n",
    "            rss=np.random.randint(0,100000,n).tolist()\n",
    "\n",
    "            exp_name=f\"markov_mixture/{group_name}/{model_name}/n={n}\"#dense,2,3\n",
    "            config_=copy.deepcopy(config)\n",
    "            config_[\"exp_dir\"]=f\"./data/{exp_name}\"\n",
    "            config_[\"wandb\"]={\"project\":\"markov_mixture\",\n",
    "            \"group\":group_name,\n",
    "            \"name\":f\"{model_name}_n={n}\"}\n",
    "\n",
    "            config_[\"data_params\"]={\n",
    "                \"ks\":ks,\n",
    "                \"ps\":ps,\n",
    "                \"transition_paramss\":[{\"type\":\"dense\",\"random_state\":rs} for rs in rss],\n",
    "                \"l\":l,\n",
    "                \"batch_size\":batch_size,\n",
    "                \"fast_dataset\":True,\n",
    "                \"two_token_input\":False if \"lte\" not in variant else True,\n",
    "            }\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_embd\"]=n_embd\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_head\"]=n_head\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_layer\"]=n_layer\n",
    "            if \"rope_only\" in variant:\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"rope\"]=True\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"pos_embed\"]=False\n",
    "\n",
    "\n",
    "            ## set n_steps\n",
    "            flops_per_step=3*(batch_size)*utils.get_nano_gpt_forward_flops(gpt_config=config_[\"model_params\"][\"gpt_config\"],seq_len=l)\n",
    "            n_steps=int(flops_total/flops_per_step)\n",
    "            config_[\"memo\"]={\"flops_per_step\":flops_per_step,}\n",
    "            config_[\"training_params\"][\"n_steps\"]=n_steps\n",
    "            config_[\"training_params\"][\"save_steps\"]=get_ckpt_steps(n_steps)\n",
    "\n",
    "            yaml_path=os.path.join(f\"./yamls/{exp_name}.yaml\")\n",
    "            fol=os.path.dirname(yaml_path)\n",
    "            os.makedirs(fol,exist_ok=True)\n",
    "            yaml.dump(config_, open(yaml_path, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rmlps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=[4,8,16,32,64,128,256,512,1024,2048]\n",
    "k=10\n",
    "l=512\n",
    "batch_size=128\n",
    "seed=42\n",
    "for rmlp in [0.25,1,16]:#4 is default\n",
    "    for variant in [\"default_long\",\"default_long_rope_only\"]:\n",
    "        flops_total=1e15 if \"long\" not in variant else 1e16\n",
    "        n_embd=64\n",
    "        n_head=4\n",
    "        n_layer=2\n",
    "        model_name=f\"rmlp={rmlp}\"\n",
    "        group_name=f\"final_v1/{variant}_rmlps\"\n",
    "        for n in ns:\n",
    "            np.random.seed(seed)\n",
    "            ks=[k]*n\n",
    "            ps=np.random.random(n)\n",
    "            ps/=ps.sum()\n",
    "            ps=ps.tolist()\n",
    "            rss=np.random.randint(0,100000,n).tolist()\n",
    "\n",
    "            exp_name=f\"markov_mixture/{group_name}/{model_name}/n={n}\"#dense,2,3\n",
    "            config_=copy.deepcopy(config)\n",
    "            config_[\"exp_dir\"]=f\"./data/{exp_name}\"\n",
    "            config_[\"wandb\"]={\"project\":\"markov_mixture\",\n",
    "            \"group\":group_name,\n",
    "            \"name\":f\"{model_name}_n={n}\"}\n",
    "\n",
    "            config_[\"data_params\"]={\n",
    "                \"ks\":ks,\n",
    "                \"ps\":ps,\n",
    "                \"transition_paramss\":[{\"type\":\"dense\",\"random_state\":rs} for rs in rss],\n",
    "                \"l\":l,\n",
    "                \"batch_size\":batch_size,\n",
    "                \"fast_dataset\":True,\n",
    "                \"two_token_input\":False if \"lte\" not in variant else True,\n",
    "            }\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_embd\"]=n_embd\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_head\"]=n_head\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_layer\"]=n_layer\n",
    "            if \"rope_only\" in variant:\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"rope\"]=True\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"pos_embed\"]=False\n",
    "            \n",
    "            config_[\"model_params\"][\"gpt_config\"][\"rmlp\"]=rmlp\n",
    "\n",
    "            ## set n_steps\n",
    "            flops_per_step=3*(batch_size)*utils.get_nano_gpt_forward_flops(gpt_config=config_[\"model_params\"][\"gpt_config\"],seq_len=l)\n",
    "            n_steps=int(flops_total/flops_per_step)\n",
    "            config_[\"memo\"]={\"flops_per_step\":flops_per_step,}\n",
    "            config_[\"training_params\"][\"n_steps\"]=n_steps\n",
    "            config_[\"training_params\"][\"save_steps\"]=get_ckpt_steps(n_steps)\n",
    "\n",
    "            yaml_path=os.path.join(f\"./yamls/{exp_name}.yaml\")\n",
    "            fol=os.path.dirname(yaml_path)\n",
    "            os.makedirs(fol,exist_ok=True)\n",
    "            yaml.dump(config_, open(yaml_path, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=[4,8,16,32,64,128,256,512,1024,2048]\n",
    "k=10\n",
    "l=512\n",
    "batch_size=128\n",
    "seed=42\n",
    "for mode in [\"online\"]:\n",
    "    for variant in [\"default_long\",\"default_long_rope_only\"]:\n",
    "        flops_total=1e15 if \"long\" not in variant else 1e16\n",
    "        n_embd=64\n",
    "        n_head=4\n",
    "        n_layer=2\n",
    "        model_name=f\"mode={mode}\"\n",
    "        group_name=f\"final_v1/{variant}_permutes\"\n",
    "        for n in ns:\n",
    "            np.random.seed(seed)\n",
    "            ks=[k]*n\n",
    "            ps=np.random.random(n)\n",
    "            ps/=ps.sum()\n",
    "            ps=ps.tolist()\n",
    "            rss=np.random.randint(0,100000,n).tolist()\n",
    "\n",
    "            exp_name=f\"markov_mixture/{group_name}/{model_name}/n={n}\"#dense,2,3\n",
    "            config_=copy.deepcopy(config)\n",
    "            config_[\"exp_dir\"]=f\"./data/{exp_name}\"\n",
    "            config_[\"wandb\"]={\"project\":\"markov_mixture\",\n",
    "            \"group\":group_name,\n",
    "            \"name\":f\"{model_name}_n={n}\"}\n",
    "\n",
    "            config_[\"data_params\"]={\n",
    "                \"ks\":ks,\n",
    "                \"ps\":ps,\n",
    "                \"transition_paramss\":[{\"type\":\"dense\",\"random_state\":rs} for rs in rss],\n",
    "                \"l\":l,\n",
    "                \"batch_size\":batch_size,\n",
    "                \"fast_dataset\":True,\n",
    "                \"two_token_input\":False if \"lte\" not in variant else True,\n",
    "                \"online_permute\":True if mode==\"online\" else False,\n",
    "            }\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_embd\"]=n_embd\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_head\"]=n_head\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_layer\"]=n_layer\n",
    "            if \"rope_only\" in variant:\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"rope\"]=True\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"pos_embed\"]=False\n",
    "\n",
    "            ## set n_steps\n",
    "            flops_per_step=3*(batch_size)*utils.get_nano_gpt_forward_flops(gpt_config=config_[\"model_params\"][\"gpt_config\"],seq_len=l)\n",
    "            n_steps=int(flops_total/flops_per_step)\n",
    "            config_[\"memo\"]={\"flops_per_step\":flops_per_step,}\n",
    "            config_[\"training_params\"][\"n_steps\"]=n_steps\n",
    "            config_[\"training_params\"][\"save_steps\"]=get_ckpt_steps(n_steps)\n",
    "\n",
    "            yaml_path=os.path.join(f\"./yamls/{exp_name}.yaml\")\n",
    "            fol=os.path.dirname(yaml_path)\n",
    "            os.makedirs(fol,exist_ok=True)\n",
    "            yaml.dump(config_, open(yaml_path, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=[4,8,16,32,64,128,256,512,1024,2048]\n",
    "k=10\n",
    "l=512\n",
    "batch_size=128\n",
    "seed=42\n",
    "for nhead in [1,8,16]:\n",
    "    for variant in [\"default_long\",\"default_long_rope_only\"]:\n",
    "        flops_total=1e15 if \"long\" not in variant else 1e16\n",
    "        n_embd=64\n",
    "        n_head=nhead\n",
    "        n_layer=2\n",
    "        model_name=f\"nhead={nhead}\"\n",
    "        group_name=f\"final_v1/{variant}_nheads\"\n",
    "        for n in ns:\n",
    "            np.random.seed(seed)\n",
    "            ks=[k]*n\n",
    "            ps=np.random.random(n)\n",
    "            ps/=ps.sum()\n",
    "            ps=ps.tolist()\n",
    "            rss=np.random.randint(0,100000,n).tolist()\n",
    "\n",
    "            exp_name=f\"markov_mixture/{group_name}/{model_name}/n={n}\"#dense,2,3\n",
    "            config_=copy.deepcopy(config)\n",
    "            config_[\"exp_dir\"]=f\"./data/{exp_name}\"\n",
    "            config_[\"wandb\"]={\"project\":\"markov_mixture\",\n",
    "            \"group\":group_name,\n",
    "            \"name\":f\"{model_name}_n={n}\"}\n",
    "\n",
    "            config_[\"data_params\"]={\n",
    "                \"ks\":ks,\n",
    "                \"ps\":ps,\n",
    "                \"transition_paramss\":[{\"type\":\"dense\",\"random_state\":rs} for rs in rss],\n",
    "                \"l\":l,\n",
    "                \"batch_size\":batch_size,\n",
    "                \"fast_dataset\":True,\n",
    "                \"two_token_input\":False if \"lte\" not in variant else True,\n",
    "            }\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_embd\"]=n_embd\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_head\"]=n_head\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_layer\"]=n_layer\n",
    "            if \"rope_only\" in variant:\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"rope\"]=True\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"pos_embed\"]=False\n",
    "\n",
    "            ## set n_steps\n",
    "            flops_per_step=3*(batch_size)*utils.get_nano_gpt_forward_flops(gpt_config=config_[\"model_params\"][\"gpt_config\"],seq_len=l)\n",
    "            n_steps=int(flops_total/flops_per_step)\n",
    "            config_[\"memo\"]={\"flops_per_step\":flops_per_step,}\n",
    "            config_[\"training_params\"][\"n_steps\"]=n_steps\n",
    "            config_[\"training_params\"][\"save_steps\"]=get_ckpt_steps(n_steps)\n",
    "\n",
    "            yaml_path=os.path.join(f\"./yamls/{exp_name}.yaml\")\n",
    "            fol=os.path.dirname(yaml_path)\n",
    "            os.makedirs(fol,exist_ok=True)\n",
    "            yaml.dump(config_, open(yaml_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops_total(flops_k10,k):\n",
    "    return flops_k10*(k/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=[4,8,16,32,64,128,256,512,1024,2048]\n",
    "for variant in [\"default_long\",\"default_long_rope_only\"]:\n",
    "    flops_total_k10=1e16 if \"long\" in variant else 1e15\n",
    "    for k in [2,4,8,16,32,64]:\n",
    "        flops_total=get_flops_total(flops_k10=flops_total_k10,k=k)\n",
    "        n_embd=64\n",
    "        n_head=4\n",
    "        n_layer=2\n",
    "        model_name=f\"k={k}\"\n",
    "        group_name=f\"final_v1/{variant}_ks\"\n",
    "        for n in ns:\n",
    "            k=k\n",
    "            l=512\n",
    "            seed=42\n",
    "            #\n",
    "            np.random.seed(seed)\n",
    "            ks=[k]*n\n",
    "            ps=np.random.random(n)\n",
    "            ps/=ps.sum()\n",
    "            ps=ps.tolist()\n",
    "            rss=np.random.randint(0,100000,n).tolist()\n",
    "\n",
    "            exp_name=f\"markov_mixture/{group_name}/{model_name}/n={n}\"#dense,2,3\n",
    "            config_=copy.deepcopy(config)\n",
    "            config_[\"exp_dir\"]=f\"./data/{exp_name}\"\n",
    "            config_[\"wandb\"]={\"project\":\"markov_mixture\",\n",
    "            \"group\":group_name,\n",
    "            \"name\":f\"{model_name}_n={n}\"}\n",
    "\n",
    "            config_[\"seed\"]=0\n",
    "            config_[\"task\"]=\"markov_mixture\"\n",
    "            config_[\"data_params\"]={\n",
    "                \"ks\":ks,\n",
    "                \"ps\":ps,\n",
    "                \"transition_paramss\":[{\"type\":\"dense\",\"random_state\":rs} for rs in rss],\n",
    "                \"l\":l,\n",
    "                \"batch_size\":128,\n",
    "                \"fast_dataset\":True,\n",
    "                \"two_token_input\":False if \"lte\" not in variant else True,\n",
    "            }\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"in_size\"]=n_tokens\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_embd\"]=n_embd\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_head\"]=n_head\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_layer\"]=n_layer\n",
    "            if \"rope_only\" in variant:\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"rope\"]=True\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"pos_embed\"]=False\n",
    "\n",
    "            gpt_config=config_[\"model_params\"][\"gpt_config\"]\n",
    "            batch_size=config_[\"data_params\"][\"batch_size\"]\n",
    "            l=config_[\"data_params\"][\"l\"]\n",
    "            flops_per_step=3*(batch_size)*utils.get_nano_gpt_forward_flops(gpt_config,seq_len=l)\n",
    "            n_steps=(flops_total/flops_per_step)\n",
    "            n_steps=int(n_steps)\n",
    "            config_[\"memo\"]={\n",
    "                \"flops_per_step\":flops_per_step,\n",
    "            }\n",
    "            config_[\"training_params\"][\"n_steps\"]=n_steps\n",
    "            config_[\"training_params\"][\"save_steps\"]=get_ckpt_steps(n_steps)\n",
    "\n",
    "            yaml_path=os.path.join(f\"./yamls/{exp_name}.yaml\")\n",
    "            fol=os.path.dirname(yaml_path)\n",
    "            os.makedirs(fol,exist_ok=True)\n",
    "            yaml.dump(config_, open(yaml_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=[4,8,16,32,64,128,256,512,1024,2048]\n",
    "k=10\n",
    "batch_size=128\n",
    "seed=42\n",
    "for l in [64,128,256,512,1024,2048]:\n",
    "    for variant in [\"default_long_rope_only\"]:\n",
    "        flops_total=1e15 if \"long\" not in variant else 1e16\n",
    "        n_embd=64\n",
    "        n_head=4\n",
    "        n_layer=2\n",
    "        model_name=f\"l={l}\"\n",
    "        group_name=f\"final_v1/{variant}_ls\"\n",
    "        for n in ns:\n",
    "            np.random.seed(seed)\n",
    "            ks=[k]*n\n",
    "            ps=np.random.random(n)\n",
    "            ps/=ps.sum()\n",
    "            ps=ps.tolist()\n",
    "            rss=np.random.randint(0,100000,n).tolist()\n",
    "\n",
    "            exp_name=f\"markov_mixture/{group_name}/{model_name}/n={n}\"#dense,2,3\n",
    "            config_=copy.deepcopy(config)\n",
    "            config_[\"exp_dir\"]=f\"./data/{exp_name}\"\n",
    "            config_[\"wandb\"]={\"project\":\"markov_mixture\",\n",
    "            \"group\":group_name,\n",
    "            \"name\":f\"{model_name}_n={n}\"}\n",
    "\n",
    "            config_[\"data_params\"]={\n",
    "                \"ks\":ks,\n",
    "                \"ps\":ps,\n",
    "                \"transition_paramss\":[{\"type\":\"dense\",\"random_state\":rs} for rs in rss],\n",
    "                \"l\":l,\n",
    "                \"batch_size\":batch_size,\n",
    "                \"fast_dataset\":True,\n",
    "                \"two_token_input\":False if \"lte\" not in variant else True,\n",
    "            }\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_embd\"]=n_embd\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_head\"]=n_head\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"n_layer\"]=n_layer\n",
    "            config_[\"model_params\"][\"gpt_config\"][\"block_size\"]=l if l>1024 else 1024\n",
    "            if \"rope_only\" in variant:\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"rope\"]=True\n",
    "                config_[\"model_params\"][\"gpt_config\"][\"pos_embed\"]=False\n",
    "            \n",
    "\n",
    "            ## set n_steps\n",
    "            flops_per_step=3*(batch_size)*utils.get_nano_gpt_forward_flops(gpt_config=config_[\"model_params\"][\"gpt_config\"],seq_len=l)\n",
    "            #3 for training\n",
    "            n_steps=int(flops_total/flops_per_step)\n",
    "            config_[\"memo\"]={\"flops_per_step\":flops_per_step,}\n",
    "            config_[\"training_params\"][\"n_steps\"]=n_steps\n",
    "            config_[\"training_params\"][\"save_steps\"]=get_ckpt_steps(n_steps)\n",
    "\n",
    "            yaml_path=os.path.join(f\"./yamls/{exp_name}.yaml\")\n",
    "            fol=os.path.dirname(yaml_path)\n",
    "            os.makedirs(fol,exist_ok=True)\n",
    "            yaml.dump(config_, open(yaml_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
